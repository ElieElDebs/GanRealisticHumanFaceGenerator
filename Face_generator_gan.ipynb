{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "from tensorflow.keras.layers import Conv2D, Dense, BatchNormalization, Flatten, InputLayer, LeakyReLU, Conv2DTranspose, Reshape  \n",
    "from tensorflow.keras import Model\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "import os\n",
    "import time"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Dataset Description\n",
    "\n",
    "##### Human Faces Dataset (128x128x3)\n",
    "\n",
    "This dataset is a comprehensive collection of over 50,000 images, exclusively focusing on human faces. The images in this dataset are of size 128 pixels in width and 128 pixels in height. Each image represents a human face captured from various individuals, encompassing diverse expressions, poses, and ethnicities. However, the dataset contains several inherent challenges:\n",
    "\n",
    "1. *Off-center faces*: Not all faces in the images are perfectly centered. Some images may have faces positioned towards the edges or corners.\n",
    "\n",
    "2. *Multiple faces*: Occasionally, an image may contain multiple faces. This includes scenarios where two or more individuals are captured in a single image.\n",
    "\n",
    "3. *Occluding objects*: In some images, objects or obstructions may partially or completely occlude the faces. These occluding objects can include accessories like hats, scarves, or sunglasses, as well as other objects that unintentionally obstruct the facial features.\n",
    "\n",
    "4. *Overexposure*: Some images in the dataset may suffer from overexposure due to excessive lighting conditions. These overexposed images can affect the visibility and quality of facial features, posing a challenge for face recognition and analysis algorithms.\n",
    "\n",
    "5. *Non-uniform backgrounds*: The backgrounds of the images are not consistent across the dataset. The variations in backgrounds include different colors, textures, and environmental settings. This diversity in backgrounds introduces additional complexity for tasks such as face segmentation and background removal.\n",
    "\n",
    "##### Dataset Contents\n",
    "\n",
    "The dataset includes:\n",
    "\n",
    "- Over 50,000 high-resolution images of human faces.\n",
    "- Images are stored in a standardized format of 128x128 pixels.\n",
    "- Each image is meticulously curated and annotated for accurate analysis.\n",
    "   \n",
    "![Portion of the dataset](./data/illustration/dataset_illustration.png)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Data Preprocessing\n",
    "\n",
    "Data preprocessing is an essential step in preparing data for analysis or machine learning tasks. It involves transforming and cleaning the raw data to make it suitable for further processing. Let's explore the specific steps involved in this process:\n",
    "\n",
    "##### 1. Data Acquisition\n",
    "\n",
    "The first step is to read the 50,000 images. This may involve loading the images from a local directory, a remote server, or a dataset repository. The goal is to have the raw data ready for further processing. In our case, we're reading a local directory.\n",
    "\n",
    "##### 2. Data Normalization\n",
    "\n",
    "To ensure consistency and improved model convergence, it's important to normalize the values of the images. In our case, we're normalizing the pixel values between -1 and 1.\n",
    "\n",
    "##### 3. Shuffle and Batch Creation\n",
    "\n",
    "To introduce randomness and prevent any potential biases in the data ordering, we can shuffle the images before creating batches. This ensures that the order of the images does not influence the learning process. After shuffling, we can proceed to create batches of 32 images.\n",
    "\n",
    "##### 4. Batch Processing\n",
    "\n",
    "To efficiently process large datasets, it's common to divide them into smaller batches. In this case, we have created batches of 32 images. This allows us to feed the data in smaller portions during training or analysis, reducing memory requirements and enabling potentially enabling parallel processing if available.\n",
    "\n",
    "By dividing the data into batches, we can iteratively process each batch without loading the entire dataset into memory at once. This approach is particularly useful when working with large datasets that do not fit entirely into memory.\n",
    "\n",
    "These steps ensure that the data is properly prepared for subsequent analysis or machine learning tasks. It's important to note that these steps can be customized based on specific requirements and the nature of the dataset.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_and_preprocess_dataset(directory, image_size, batch_size, shuffle=True):\n",
    "    \"\"\"\n",
    "    function that reads the Dataset, Shuffle it, normalizes it and batches it \n",
    "    \"\"\"\n",
    "    dataset = tf.keras.utils.image_dataset_from_directory(\n",
    "        directory=directory,\n",
    "        image_size=image_size,\n",
    "        batch_size=batch_size,\n",
    "        shuffle=shuffle,\n",
    "        label_mode=None, \n",
    "        color_mode=\"rgb\"  # RGB Color\n",
    "    )\n",
    "\n",
    "    # # Normalizing Dataset between -1 and 1\n",
    "    dataset = dataset.map(lambda x: x / 255.0 * 2.0 - 1.0)\n",
    "\n",
    "    return dataset\n",
    "\n",
    "TRAIN_DIR = \"./data/data/train/real\"\n",
    "IMAGE_SIZE = (128,128)\n",
    "BATCH_SIZE = 32\n",
    "\n",
    "# Chargement des données d'entraînement\n",
    "train_data = load_and_preprocess_dataset(TRAIN_DIR, IMAGE_SIZE, BATCH_SIZE, shuffle=True)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Discriminator\n",
    "\n",
    "Next step is to create The discriminator. The discriminator is a key component of a Generative Adversarial Network (GAN). Its role is to distinguish between real and generated (fake) samples. Here's a brief description of the discriminator:\n",
    "\n",
    "- Input: The discriminator takes as input images or samples from the generator. In the case of image generation tasks, the input typically consists of images with 128*128*3 dimensions.\n",
    "\n",
    "- Architecture: The discriminator usually consists of layers that process the input and extract features. Common architectural choices include convolutional layers, followed by activation functions such as ReLU or LeakyReLU. These layers are designed to capture relevant patterns and discriminative information from the input samples.\n",
    "\n",
    "- Output: The output of the discriminator is a probability score that represents the likelihood of the input being real or fake. It is usually a single scalar value between 0 and 1. A value closer to 1 indicates that the input is classified as real, while a value closer to 0 suggests that it is classified as fake.\n",
    "\n",
    "The discriminator's objective is to improve its ability to differentiate between real and generated samples, which in turn drives the generator to produce more realistic outputs. The discriminator and generator are trained in an adversarial manner, where the generator aims to fool the discriminator, while the discriminator aims to accurately classify the samples."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Discriminator (Model):\n",
    "    def __init__(self, input_shape, batch_size):\n",
    "        super(Discriminator, self).__init__()\n",
    "\n",
    "        self.__input_shape = input_shape\n",
    "        self.__batch_size = batch_size\n",
    "\n",
    "        self.__discriminator = tf.keras.Sequential(\n",
    "            [\n",
    "                InputLayer(input_shape=self.__input_shape, batch_size=self.__batch_size),\n",
    "                Conv2D(64,(3,3),padding=\"same\", strides=2),\n",
    "                BatchNormalization(),\n",
    "                Conv2D(64,(3,3),padding=\"same\", strides=2),\n",
    "                BatchNormalization(),\n",
    "                LeakyReLU(alpha=0.01),\n",
    "                Conv2D(128,(3,3),padding=\"same\", strides=2),\n",
    "                BatchNormalization(),\n",
    "                LeakyReLU(alpha=0.01),\n",
    "                Conv2D(256,(3,3),padding=\"same\", strides=2),\n",
    "                BatchNormalization(),\n",
    "                LeakyReLU(alpha=0.01),\n",
    "                Flatten(),\n",
    "                Dense (1, activation =\"sigmoid\")\n",
    "            ]\n",
    "        )\n",
    "\n",
    "    def call (self, input) :\n",
    "        return self.__discriminator(input)\n",
    "\n",
    "    def get_model (self) :\n",
    "        return self.__discriminator"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Generator\n",
    "\n",
    "The generator is a fundamental component of a Generative Adversarial Network (GAN). Its purpose is to generate synthetic samples that resemble real data. Let's take a quick look at the generator:\n",
    "\n",
    "- Input: The generator typically takes random noise or a latent vector as input. This vector is often sampled from a probability distribution, such as a uniform or Gaussian distribution. The size and dimensionality of the input vector depend on the specific problem and the desired output.\n",
    "\n",
    "- Architecture: The generator consists of layers that transform the input noise or latent vector into meaningful data representations. Common choices include fully connected (dense) layers or transposed convolutional layers. These layers gradually upsample the input and apply non-linearities to generate higher-resolution outputs.\n",
    "\n",
    "- Output: The output of the generator is a synthetic sample that aims to resemble the real data. In image generation tasks, the output can be an image with specific dimensions, such as 32x32 or 64x64 pixels. The generator generates samples that lie within the same data distribution as the real samples.\n",
    "\n",
    "- Training: The generator is trained in conjunction with the discriminator. Its objective is to produce synthetic samples that can fool the discriminator into classifying them as real. The generator's parameters are updated through backpropagation and optimization techniques, such as stochastic gradient descent (SGD) or Adam, based on the feedback from the discriminator.\n",
    "\n",
    "The generator's goal is to progressively improve its ability to generate realistic and diverse samples. It learns to capture the underlying patterns and structures present in the real data, effectively synthesizing new samples from random noise. As the training progresses, the generator becomes more proficient in generating samples that resemble the true data distribution.\n",
    "\n",
    "In summary, the generator in a GAN framework acts as a creative component, producing synthetic samples that progressively become more indistinguishable from real data. It plays a crucial role in challenging the discriminator and enhancing the overall performance of the GAN model.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Generator (Model):\n",
    "    def __init__(self, batch_size):\n",
    "        super(Generator, self).__init__()\n",
    "\n",
    "        self.__generator = tf.keras.Sequential(\n",
    "            [\n",
    "                InputLayer(input_shape=(64,), batch_size=batch_size),\n",
    "                Dense (8*8*64, use_bias=False),\n",
    "                LeakyReLU(alpha=0.01),\n",
    "                Reshape ((8,8,64)),\n",
    "                Conv2DTranspose(filters=256, padding=\"same\", kernel_size=(3,3), strides = 2),\n",
    "                BatchNormalization (),\n",
    "                LeakyReLU(alpha=0.01),\n",
    "                Conv2DTranspose(filters=256, padding=\"same\", kernel_size=(3,3), strides = 2),\n",
    "                BatchNormalization (),\n",
    "                LeakyReLU(alpha=0.01),\n",
    "                Conv2DTranspose(filters=256, padding=\"same\", kernel_size=(3,3), strides = 2),\n",
    "                BatchNormalization (),\n",
    "                LeakyReLU(alpha=0.01),\n",
    "                Conv2DTranspose(filters=128, padding=\"same\", kernel_size=(3,3), strides = 2),\n",
    "                BatchNormalization (),\n",
    "                LeakyReLU(alpha=0.01),\n",
    "                Conv2D(3,(3,3),padding=\"same\")\n",
    "            ]\n",
    "        )\n",
    "\n",
    "    def call (self, input) :\n",
    "        return self.__generator(input)\n",
    "\n",
    "    def get_model (self) :\n",
    "        return self.__generator"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dis = Discriminator((128,128,3),BATCH_SIZE)\n",
    "gen = Generator(BATCH_SIZE)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "###  Training "
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Define the loss functions: Specify the loss functions for the generator and discriminator. The discriminator's loss aims to correctly classify real and fake samples, while the generator's loss encourages the generated samples to be classified as real."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "cross_entropy = tf.keras.losses.BinaryCrossentropy()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "- Discriminator loss\n",
    "\n",
    "This function calculates the loss for the discriminator during training. It takes two inputs: `real_output` and `fake_output`, which represent the discriminator's predictions for real and fake samples, respectively.\n",
    "\n",
    "The function uses the `cross_entropy` loss function to compare the discriminator's predictions to the target labels. For real samples, the target label is 1, indicating that the sample is real. For fake samples, the target label is 0, indicating that the sample is fake.\n",
    "\n",
    "The `real_loss` is computed by comparing the real output predictions to a tensor of ones with the same shape as `real_output`. This loss represents how well the discriminator can correctly classify real samples.\n",
    "\n",
    "Similarly, the `fake_loss` is computed by comparing the fake output predictions to a tensor of zeros with the same shape as `fake_output`. This loss represents how well the discriminator can correctly classify fake samples.\n",
    "\n",
    "Finally, the `total_loss` is calculated by summing the `real_loss` and `fake_loss`. This combined loss is used to optimize the discriminator's weights during training.\n",
    "\n",
    "The function then returns the `total_loss`, which represents the overall loss of the discriminator."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "def discriminator_loss(real_output, fake_output):\n",
    "    \n",
    "    real_loss = cross_entropy(tf.ones_like(real_output), real_output)\n",
    "    fake_loss = cross_entropy(tf.zeros_like(fake_output), fake_output)\n",
    "    total_loss = real_loss + fake_loss\n",
    "    \n",
    "    return total_loss"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "- Generator loss\n",
    "\n",
    "This function calculates the loss for the generator in a Generative Adversarial Network (GAN) during training. It takes `fake_output` as input, which represents the discriminator's predictions for the generated (fake) samples.\n",
    "\n",
    "The function uses the `cross_entropy` loss function to compare the discriminator's predictions for the fake samples to a tensor of ones with the same shape as `fake_output`. The target label is 1, indicating that the desired outcome for the generator is to fool the discriminator into classifying the fake samples as real.\n",
    "\n",
    "The `gen_loss` is computed by comparing the fake output predictions to the tensor of ones. This loss represents how well the generator is performing in generating samples that can deceive the discriminator.\n",
    "\n",
    "Finally, the function returns the `gen_loss` as a float value."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "def generator_loss(fake_output):\n",
    "    \n",
    "    gen_loss = cross_entropy(tf.ones_like(fake_output), fake_output)\n",
    "\n",
    "    return float (gen_loss)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "generator_optimizer = tf.keras.optimizers.Adam(1e-4)\n",
    "discriminator_optimizer = tf.keras.optimizers.Adam(1e-4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "checkpoint_dir = './training_checkpoints_V3'\n",
    "checkpoint_prefix = os.path.join(checkpoint_dir, \"ckpt\")\n",
    "checkpoint = tf.train.Checkpoint(generator_optimizer=generator_optimizer,\n",
    "                                 discriminator_optimizer=discriminator_optimizer,\n",
    "                                 generator=gen,\n",
    "                                 discriminator=dis)\n",
    "\n",
    "ckpt_manager = tf.train.CheckpointManager(checkpoint, checkpoint_dir, max_to_keep=3)\n",
    "if ckpt_manager.latest_checkpoint:\n",
    "    checkpoint.restore(ckpt_manager.latest_checkpoint)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "EPOCHS = 50\n",
    "noise_dim = 64\n",
    "num_examples_to_generate = 16\n",
    "\n",
    "# You will reuse this seed overtime (so it's easier)\n",
    "# to visualize progress in the animated GIF)\n",
    "seed = tf.random.normal([num_examples_to_generate, noise_dim])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Notice the use of `tf.function`\n",
    "# This annotation causes the function to be \"compiled\".\n",
    "@tf.function\n",
    "def train_step(images):\n",
    "    noise = tf.random.normal([BATCH_SIZE, noise_dim])\n",
    "\n",
    "    with tf.GradientTape() as gen_tape, tf.GradientTape() as disc_tape:\n",
    "      generated_images = gen(noise, training=True)\n",
    "\n",
    "      real_output = dis(images, training=True)\n",
    "      fake_output = dis(generated_images, training=True)\n",
    "\n",
    "      gen_loss = generator_loss(fake_output)\n",
    "      disc_loss = discriminator_loss(real_output, fake_output)\n",
    "\n",
    "    gradients_of_generator = gen_tape.gradient(gen_loss, gen.trainable_variables)\n",
    "    gradients_of_discriminator = disc_tape.gradient(disc_loss, dis.trainable_variables)\n",
    "\n",
    "    generator_optimizer.apply_gradients(zip(gradients_of_generator, gen.trainable_variables))\n",
    "    discriminator_optimizer.apply_gradients(zip(gradients_of_discriminator, dis.trainable_variables))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def gen_and_show_image (model, epoch, noise) :\n",
    "    gen_image = model(noise, training=False)\n",
    "    plt.imshow(gen_image[0])\n",
    "    plt.axis('off')\n",
    "    plt.savefig('./images/image_at_epoch_{:04d}.png'.format(epoch))\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train(dataset, epochs):\n",
    "  for epoch in range(epochs):\n",
    "    start = time.time()\n",
    "\n",
    "    for image_batch in dataset:\n",
    "      train_step(image_batch)\n",
    "      # Generate after the final epoch\n",
    "    \n",
    "    gen_and_show_image(\n",
    "        gen,\n",
    "        epoch,\n",
    "        seed\n",
    "      )\n",
    "\n",
    "    checkpoint.save(file_prefix = checkpoint_prefix)\n",
    "\n",
    "    print ('Time for epoch {} is {} sec'.format(epoch + 1, time.time()-start))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train(train_data, EPOCHS)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Result\n",
    "\n",
    "![Training evolution](./data/illustration/training.gif)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.6"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
